{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import  torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as td"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义变量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_works = 4\n",
    "batch_size = 64\n",
    "input_dim= 100\n",
    "epochs = 25\n",
    "print(\"run on \",device.type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "\n",
    "# loading data\n",
    "def loading_minist_data(batch_siz:int, works:int)->(Iterable,Iterable):\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(root=\"./data\",train=True,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "    test_set = torchvision.datasets.MNIST(root=\"./data\",train=False,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "    return td.DataLoader(dataset=train_set, batch_size=batch_siz, shuffle=True, num_workers=works,drop_last=True), td.DataLoader(dataset=test_set, batch_size=batch_siz, shuffle=True, num_workers=works,drop_last=True)\n",
    "\n",
    "train_iter ,test_iter = loading_minist_data(batch_size,4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iter len  937\n",
      "all size  59968\n"
     ]
    }
   ],
   "source": [
    "print(\"train_iter len \",len(train_iter))\n",
    "print(\"all size \",len(train_iter)*batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape,\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1,32,kernel_size=3,stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32,64,kernel_size=3,padding=1,stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64,128,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*3*3,1),\n",
    "            nn.Sigmoid(),\n",
    "        ).to(device)\n",
    "        self.cnt=0\n",
    "        self.running_loss=0\n",
    "        self.loss_func = nn.BCELoss()\n",
    "        self.loss_metric=[]\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=0.001)\n",
    "        pass\n",
    "    def forward(self,inputs):\n",
    "        for op in  self.model:\n",
    "            # print(inputs.shape)\n",
    "            inputs = op(inputs)\n",
    "        return inputs\n",
    "    def train(self,inputs,targets):\n",
    "        out = self.forward(inputs)\n",
    "        loss = self.loss_func(out,targets)\n",
    "        self.running_loss += loss.detach().item()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        pass\n",
    "    def evaluate_loss(self):\n",
    "        ts = self.running_loss/(len(train_iter)*batch_size)\n",
    "        self.loss_metric.append(ts)\n",
    "\n",
    "        self.running_loss=0\n",
    "\n",
    "        pass\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,in_dim:int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # nn.Linear(in_dim, 200),\n",
    "            # nn.LeakyReLU(0.02),\n",
    "            # # nn.LayerNorm(200),\n",
    "            # nn.LayerNorm(200),\n",
    "            # nn.Linear(200, 784),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Linear(256,28*28),\n",
    "            # nn.Tanh()\n",
    "            nn.Linear(in_dim,256*7*7),\n",
    "            View( shape=(batch_size,256,7,7)),\n",
    "            # 256*7*7 -> 128*14*14\n",
    "            # 7 + (2-1)*6 +\n",
    "            nn.ConvTranspose2d(256,128,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.ConvTranspose2d(128,64,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.ConvTranspose2d(64,1,kernel_size=4,padding=1,stride=2),\n",
    "            nn.Tanh()\n",
    "        ).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        # counter and accumulator for progress\n",
    "        self.loss_metric = []\n",
    "        self.running_loss =0\n",
    "    def forward(self,inputs):\n",
    "        for op in  self.model:\n",
    "            # print(inputs.shape)\n",
    "            inputs = op(inputs)\n",
    "        return  inputs\n",
    "    def train(self,D:Discriminator,inputs,targets):\n",
    "        gen_out = self.forward(inputs)\n",
    "        # print(gen_out.shape)\n",
    "        d_out = D.forward(gen_out)\n",
    "        loss  = D.loss_func(d_out,targets)\n",
    "        self.running_loss += loss.detach().item()\n",
    "        # print(self.running_loss)\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "    def evaluate_loss(self):\n",
    "        print(\"G running loss \",self.running_loss)\n",
    "        ts = self.running_loss/(len(train_iter)*batch_size)\n",
    "        self.loss_metric.append(ts)\n",
    "        self.running_loss=0\n",
    "        pass\n",
    "def generate_random_seed(size):\n",
    "    random_data = torch.randn(size*batch_size).view(batch_size,size)\n",
    "    return random_data.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[-0.0420,  0.5540,  0.6050,  ...,  0.2987,  0.2644, -0.0994],\n          [-0.5760, -0.9815,  0.9779,  ..., -0.7733,  0.8431, -0.8172],\n          [ 0.8521, -0.4639,  0.6512,  ...,  0.9939,  0.7953,  0.8148],\n          ...,\n          [ 0.1751,  0.5316, -0.9008,  ...,  0.9432, -0.8040, -0.7854],\n          [ 0.6957,  0.3986,  0.8890,  ...,  0.1278, -0.1684,  0.9163],\n          [ 0.0194, -0.2758, -0.4726,  ...,  0.7095, -0.3093, -0.1259]]],\n\n\n        [[[ 0.4800,  0.8162,  0.5662,  ...,  0.2098,  0.1138, -0.0812],\n          [ 0.1374, -0.9668, -0.2674,  ..., -0.3859, -0.3530, -0.7552],\n          [ 0.9877,  0.8095,  0.7612,  ...,  0.9096,  0.8294,  0.2942],\n          ...,\n          [ 0.4237, -0.7836,  0.8427,  ..., -0.6134, -0.3697, -0.9293],\n          [ 0.7906,  0.9634,  0.9709,  ...,  0.6769,  0.7210,  0.7279],\n          [-0.6092,  0.8047, -0.1349,  ..., -0.0358, -0.4291,  0.4439]]],\n\n\n        [[[-0.2819, -0.2601,  0.7228,  ..., -0.8839,  0.7975,  0.4222],\n          [-0.3489,  0.5866,  0.8259,  ...,  0.8953,  0.9153,  0.4198],\n          [ 0.9405,  0.7489,  0.4959,  ...,  0.8454, -0.2503,  0.8102],\n          ...,\n          [-0.1908,  0.0367,  0.1117,  ..., -0.1365, -0.4695,  0.1871],\n          [ 0.8692,  0.4865,  0.2533,  ...,  0.9747, -0.5268,  0.3130],\n          [-0.2371,  0.7129, -0.8184,  ...,  0.7008, -0.7469, -0.3241]]],\n\n\n        ...,\n\n\n        [[[-0.2470,  0.4431, -0.2413,  ...,  0.7059,  0.8102, -0.2913],\n          [ 0.7065,  0.6630,  0.8200,  ..., -0.8024,  0.9732,  0.7685],\n          [ 0.5830,  0.9479,  0.7412,  ...,  0.9509,  0.4656,  0.1642],\n          ...,\n          [-0.5509, -0.5544,  0.6903,  ...,  0.2478,  0.1132, -0.3687],\n          [ 0.8482,  0.0824, -0.3757,  ...,  0.2983,  0.3227, -0.6810],\n          [-0.4869, -0.1910, -0.6006,  ...,  0.7388, -0.3626, -0.3119]]],\n\n\n        [[[ 0.0079,  0.1698,  0.5375,  ...,  0.0753,  0.7186, -0.2343],\n          [-0.8442, -0.9946,  0.7704,  ..., -0.4281,  0.2074,  0.3123],\n          [ 0.8118,  0.8310,  0.3530,  ...,  0.8960,  0.8096,  0.9525],\n          ...,\n          [-0.1161, -0.8352,  0.9654,  ..., -0.4298,  0.9231, -0.6942],\n          [ 0.2550,  0.9762,  0.7128,  ...,  0.9595, -0.7075,  0.9818],\n          [-0.8015, -0.6778,  0.8246,  ...,  0.7743,  0.0142, -0.3845]]],\n\n\n        [[[ 0.5284,  0.7960,  0.6855,  ..., -0.3137,  0.8667, -0.3409],\n          [-0.6706, -0.8366,  0.9707,  ..., -0.9946,  0.9007, -0.7031],\n          [ 0.6313, -0.4679,  0.6378,  ...,  0.4860, -0.8542,  0.8461],\n          ...,\n          [-0.4701, -0.3437,  0.4819,  ..., -0.9422,  0.9638, -0.7449],\n          [ 0.4262,  0.9984, -0.4045,  ...,  0.9798,  0.6544,  0.3065],\n          [-0.9046, -0.4267,  0.4450,  ...,  0.1181, -0.3007, -0.1667]]]],\n       grad_fn=<TanhBackward0>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(in_dim=input_dim)\n",
    "G.forward(generate_random_seed(input_dim))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m     y \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m64\u001B[39m,\u001B[38;5;241m1\u001B[39m),dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m      7\u001B[0m     y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m----> 8\u001B[0m     \u001B[43mD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mit\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# print(len(train_iter)*batch_size)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(D\u001B[38;5;241m.\u001B[39mrunning_loss\u001B[38;5;241m/\u001B[39m(\u001B[38;5;28mlen\u001B[39m(train_iter)\u001B[38;5;241m*\u001B[39mbatch_size))\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mDiscriminator.train\u001B[0;34m(self, inputs, targets)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m,inputs,targets):\n\u001B[0;32m---> 37\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_func(out,targets)\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mitem()\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mDiscriminator.forward\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m,inputs):\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m op \u001B[38;5;129;01min\u001B[39;00m  \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel:\n\u001B[1;32m     33\u001B[0m         \u001B[38;5;66;03m# print(inputs.shape)\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m         inputs \u001B[38;5;241m=\u001B[39m \u001B[43mop\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n",
      "File \u001B[0;32m~/mambaforge/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/mambaforge/envs/py39/lib/python3.9/site-packages/torch/nn/modules/conv.py:446\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 446\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/py39/lib/python3.9/site-packages/torch/nn/modules/conv.py:442\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    440\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    441\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 442\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "D = Discriminator()\n",
    "\n",
    "for k in  range(3):\n",
    "    for it,_ in train_iter:\n",
    "        it = it.to(device)\n",
    "        y = torch.ones(size=(64,1),dtype=torch.float32)\n",
    "        y = y.to(device)\n",
    "        D.train(it,y)\n",
    "    # print(len(train_iter)*batch_size)\n",
    "    print(D.running_loss/(len(train_iter)*batch_size))\n",
    "    D.running_loss =0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G = Generator(input_dim)\n",
    "\n",
    "print(\"generate nums is run on \",generate_random_seed(input_dim).device)\n",
    "\n",
    "output = G.forward(generate_random_seed(input_dim))\n",
    "# img = output.detach().numpy().reshape(28,28)\n",
    "plt.imshow(output.cpu().detach().numpy()[0][0], interpolation='none', cmap='Blues')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "D = Discriminator()\n",
    "G = Generator(input_dim)\n",
    "for i in range(epochs):\n",
    "    print(\"epoch  {} of {}  \".format(i+1,epochs))\n",
    "    time_start = time.time()\n",
    "    for tr_item,_ in train_iter:\n",
    "        tr_item = tr_item.to(device)\n",
    "        true_label = torch.ones(size=(batch_size,1),dtype=torch.float32).to(device)\n",
    "        fake_label = torch.zeros(size=(batch_size,1),dtype=torch.float32).to(device)\n",
    "        D.train(tr_item,true_label)\n",
    "\n",
    "        D.train(G.forward(generate_random_seed(input_dim)).detach() , fake_label)\n",
    "        #\n",
    "        G.train(D,generate_random_seed(input_dim),true_label)\n",
    "    D.evaluate_loss()\n",
    "    G.evaluate_loss()\n",
    "    print(\"spend time {}   D loss {} ,G loss {} \".format(time.time()-time_start,D.loss_metric[i],G.loss_metric[i]))\n",
    "\n",
    "        # break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = G.forward(generate_random_seed(input_dim))\n",
    "img = output.detach().cpu().numpy().reshape(64,28,28)\n",
    "plt.imshow(img[9], interpolation='none', cmap='Blues')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f,axarr = plt.subplots(2,3, figsize=(16,8))\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        output = G.forward(generate_random_seed(input_dim))\n",
    "        img = output.detach().cpu().numpy()[i*3+j].reshape(28,28)\n",
    "        axarr[i,j].imshow(img, interpolation='none', cmap='Blues')\n",
    "\n",
    "# for it,_ in train_iter:\n",
    "    # plt.imshow(it[0].reshape(28,28),cmap=\"Blues\")\n",
    "    # break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}