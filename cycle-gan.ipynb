{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from clint.textui import progress\n",
    "import requests\n",
    "from  tqdm import tqdm\n",
    "import zipfile\n",
    "import os\n",
    "proxies={\n",
    "'http': 'http://127.0.0.1:7890',\n",
    "'https': 'http://127.0.0.1:7890'  # https -> http\n",
    "}\n",
    "def get_data_from_dataset(dataset:str,zip_path):\n",
    "    horse2zebra_url = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip'\n",
    "    res = requests.get(horse2zebra_url,stream=True,proxies=proxies)\n",
    "    total_length = int(res.headers.get('content-length'))\n",
    "    chunk_size = 1024*1024\n",
    "\n",
    "    file_name = \"./%s.zip\" % dataset\n",
    "    is_download = True\n",
    "    is_extract = True\n",
    "    if os.path.exists(file_name):\n",
    "        now_size = os.path.getsize(file_name)\n",
    "        is_download=False\n",
    "        if now_size!=total_length:\n",
    "            is_download=True\n",
    "        if is_download:\n",
    "            print(\"now downloading \")\n",
    "        else:\n",
    "            print(\"had downloaded\")\n",
    "    if is_download:\n",
    "\n",
    "        with open(file_name, \"wb\") as zip_file:\n",
    "            for chunk in tqdm(iterable=res.iter_content(chunk_size=chunk_size),total=total_length/chunk_size,desc=dataset,unit='MB'):\n",
    "                # print(\"write i\")\n",
    "                zip_file.write(chunk)\n",
    "    if not os.path.exists(zip_path):\n",
    "        os.makedirs(zip_path)\n",
    "    else:\n",
    "        print('path exist')\n",
    "    full_name = zip_path+'/'+dataset\n",
    "    if os.path.exists(full_name):\n",
    "        # print(\"file ex\")\n",
    "        is_extract=False\n",
    "    # print(full_name)\n",
    "    if is_extract:\n",
    "        zf = zipfile.ZipFile(file_name,mode='r')\n",
    "        for f_n in zf.namelist():\n",
    "            zf.extract(f_n,path=zip_path)\n",
    "        zf.close()\n",
    "        print(\"extract finished \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from  torch.utils.data import  Dataset\n",
    "import torchvision\n",
    "from torchvision.transforms import  ToPILImage\n",
    "import  torchvision.transforms as transforms\n",
    "class ImageDataSet(Dataset):\n",
    "    def __init__(self,dataset :str,zip_path:str,transforms_=None):\n",
    "        self.transforms = torchvision.transforms.Compose(transforms_)\n",
    "        # zip_path='./data'\n",
    "        get_data_from_dataset(dataset,zip_path)\n",
    "        # self.files_testA = glob.glob(os.path.join(zip_path, dataset, 'testA') + '/*.*')\n",
    "        # self.files_testB = glob.glob(os.path.join(zip_path, dataset, 'testB') + '/*.*')\n",
    "        self.files_trainA = glob.glob(os.path.join(zip_path,dataset, 'trainA') + '/*.*')\n",
    "        self.files_trainB = glob.glob(os.path.join(zip_path,dataset, 'trainB') + '/*.*')\n",
    "        self.len_trainA = len(self.files_trainA)\n",
    "        self.len_trainB = len(self.files_trainB)\n",
    "        # print(self.files_trainA)\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        image_trainA = Image.open( self.files_trainA[index % self.len_trainA] ).convert('RGB')\n",
    "        image_trainB = Image.open( self.files_trainB[index % self.len_trainB] ).convert('RGB')\n",
    "\n",
    "\n",
    "        return self.transforms(image_trainA), self.transforms(image_trainB)\n",
    "\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        return max(self.len_trainA,self.len_trainB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "import torch.nn.functional as F\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [  nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features)  ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block\n",
    "        model = [   nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(input_nc, 64, 7),\n",
    "                    nn.InstanceNorm2d(64),\n",
    "                    nn.ReLU(inplace=True) ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [  nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(64, output_nc, 7),\n",
    "                    nn.Tanh() ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # A bunch of convolutions one after another\n",
    "        model = [   nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(128),\n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(256),\n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(256, 512, 4, padding=1),\n",
    "                    nn.InstanceNorm2d(512),\n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        # FCN classification layer\n",
    "        model += [nn.Conv2d(512, 1, 4, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x =  self.model(x)\n",
    "        for model in  self.model:\n",
    "            # print(x.shape)\n",
    "            x = model(x)\n",
    "        # Average pooling and flatten\n",
    "        # print(x.size())\n",
    "        # print(\"aaa\",x.size()[0])\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)\n",
    "class LambdaLR():\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch:int)->float:\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# D = Discriminator(3)\n",
    "# input_fake = torch.randint(1,size=(8,3,256,256),dtype=torch.float32)\n",
    "# D.forward(input_fake).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "## train\n",
    "is_cuda = torch.cuda.is_available()\n",
    "input_channel,output_channel = 3,3\n",
    "epochs = 2\n",
    "now_epoch = 0\n",
    "batch_size = 4\n",
    "z_path = './data'\n",
    "decay_epoch = epochs//2\n",
    "height,width = 256,256\n",
    "lr = 0.0002\n",
    "n_works = 4\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "\n",
    "G_AB = Generator(input_channel,output_channel,n_residual_blocks=3)\n",
    "G_BA = Generator(output_channel,input_channel,n_residual_blocks=3)\n",
    "D_A = Discriminator(input_channel)\n",
    "D_B = Discriminator(output_channel)\n",
    "\n",
    "print(\"cuda status : ,device\",is_cuda,device)\n",
    "if is_cuda:\n",
    "    G_AB = G_AB.cuda()\n",
    "    G_BA = G_BA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "\n",
    "# for model in D_A.model:\n",
    "#     print(model.device)\n",
    "# print(D_A.model.device)\n",
    "# loss define\n",
    "loss_GAN = torch.nn.MSELoss()\n",
    "loss_cycle = torch.nn.L1Loss()\n",
    "loss_identity = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()),\n",
    "                               lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G,lr_lambda = LambdaLR(epochs,now_epoch,decay_epoch).step )\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A,lr_lambda = LambdaLR(epochs,now_epoch,decay_epoch).step )\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B,lr_lambda = LambdaLR(epochs,now_epoch,decay_epoch).step )\n",
    "\n",
    "# Inputs & targets memory allocation\n",
    "from torch.autograd import Variable\n",
    "target_real = torch.ones(size=(batch_size,),requires_grad=False)\n",
    "target_fake = torch.zeros(size=(batch_size,),requires_grad=False)\n",
    "if is_cuda:\n",
    "    target_real = target_real.cuda()\n",
    "    target_fake = target_fake.cuda()\n",
    "# print(target_real.is_cuda)\n",
    "trans = [\n",
    "    transforms.Resize(int(256*1.12), Image.BICUBIC),\n",
    "                transforms.RandomCrop(256),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ]\n",
    "horse_zebra_dataset = ImageDataSet(dataset='horse2zebra',zip_path=z_path,transforms_=trans)\n",
    "train_loader = torch.utils.data.DataLoader(horse_zebra_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epo in range(now_epoch,epochs):\n",
    "    time_start = time.time()\n",
    "    print(\"train epoch {} of {} \".format(epo,epochs) )\n",
    "    for i,(t_A,t_B) in enumerate(train_loader):\n",
    "\n",
    "        if is_cuda:\n",
    "            t_A = t_A.cuda()\n",
    "            t_B = t_B.cuda()\n",
    "#          train gen\n",
    "        print(t_A.shape)\n",
    "        optimizer_G.zero_grad()\n",
    "        same_B = G_AB(t_B)\n",
    "        same_A = G_AB(t_A)\n",
    "\n",
    "        loss_identity_B = loss_identity(same_B,t_B) * 5\n",
    "        loss_identity_A = loss_identity(same_A,t_A) * 5\n",
    "\n",
    "        fake_B = G_AB(t_A)\n",
    "        fake_A = G_BA(t_B)\n",
    "        pred_fake_B = D_B(fake_B)\n",
    "        pred_fake_A = D_A(fake_A)\n",
    "\n",
    "        loss_GAN_AB = loss_GAN(pred_fake_B,target_real)\n",
    "        loss_GAN_BA = loss_GAN(pred_fake_A,target_real)\n",
    "\n",
    "        # cycyle loss\n",
    "        recovered_A = G_BA(fake_B)\n",
    "        recovered_B = G_AB(fake_A)\n",
    "        loss_cycle_A = loss_cycle(recovered_A,t_A) * 10.0\n",
    "        loss_cycle_B = loss_cycle(recovered_B,t_B) * 10.0\n",
    "\n",
    "        loss_G = loss_identity_A +loss_GAN_AB+loss_cycle_A+loss_cycle_B+loss_GAN_BA+loss_identity_B\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "\n",
    "        # D_A train\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = D_A(t_A)\n",
    "        loss_D_real = loss_GAN(pred_real, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        # fake_A = fake_A_buffer.push_and_pop(fake_A)\n",
    "        fake_AA = G_BA(t_B)\n",
    "        pred_fake = D_A(fake_AA.detach())\n",
    "        loss_D_fake = loss_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_A.backward()\n",
    "\n",
    "        optimizer_D_A.step()\n",
    "        ###################################\n",
    "\n",
    "        ###### Discriminator B ######\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = D_B(t_B)\n",
    "        loss_D_real = loss_GAN(pred_real, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        fake_BB = G_AB(t_A)\n",
    "        pred_fake = D_B(fake_BB.detach())\n",
    "        loss_D_fake = loss_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
    "        loss_D_B.backward()\n",
    "\n",
    "        optimizer_D_B.step()\n",
    "        ###################################\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "    print(\"time spend {}\".format(time.time()-time_start))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}